:py:mod:`teehr.queries.duckdb_database`
=======================================

.. py:module:: teehr.queries.duckdb_database

.. autoapi-nested-parse::

   A module defining duckdb sql queries for a persistent database.

   ..
       !! processed by numpydoc !!


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   teehr.queries.duckdb_database.create_get_metrics_query
   teehr.queries.duckdb_database.create_join_and_save_timeseries_query
   teehr.queries.duckdb_database.describe_timeseries
   teehr.queries.duckdb_database.create_get_joined_timeseries_query
   teehr.queries.duckdb_database.create_get_timeseries_query
   teehr.queries.duckdb_database.create_get_timeseries_char_query
   teehr.queries.duckdb_database.create_unique_field_values_query



Attributes
~~~~~~~~~~

.. autoapisummary::

   teehr.queries.duckdb_database.SQL_DATETIME_STR_FORMAT


.. py:data:: SQL_DATETIME_STR_FORMAT
   :value: '%Y-%m-%d %H:%M:%S'

   

.. py:function:: create_get_metrics_query(mq: teehr.models.queries_database.MetricQuery) -> str

   
   Build the query string to calculate performance metrics
   using database queries.


   :Parameters:

       **mq** : MetricQuery
           Pydantic model containing query parameters.

   :Returns:

       str
           The query string.








   .. rubric:: Notes

   Filter, Order By and Group By Fields:

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name
   * lead_time
   * [any user-added fields]

   Basic Metrics:

   * primary_count
   * secondary_count
   * primary_minimum
   * secondary_minimum
   * primary_maximum
   * secondary_maximum
   * primary_average
   * secondary_average
   * primary_sum
   * secondary_sum
   * primary_variance
   * secondary_variance
   * max_value_delta

     * max(secondary_value) - max(primary_value)
   * bias

     * sum(primary_value - secondary_value)/count(*)

   HydroTools Metrics:

   * nash_sutcliffe_efficiency
   * kling_gupta_efficiency
   * coefficient_of_extrapolation
   * coefficient_of_persistence
   * mean_error
   * mean_squared_error
   * root_mean_squared_error

   Time-based Metrics:

   * primary_max_value_time
   * secondary_max_value_time
   * max_value_timedelta


   .. rubric:: Examples

   >>> order_by = ["lead_time", "primary_location_id"]

   >>> group_by = ["lead_time", "primary_location_id"]

   >>> filters = [
   >>>     {
   >>>         "column": "primary_location_id",
   >>>         "operator": "=",
   >>>         "value": "gage-A",
   >>>     },
   >>>     {
   >>>         "column": "reference_time",
   >>>         "operator": "=",
   >>>         "value": "2022-01-01 00:00:00",
   >>>     },
   >>>     {"column": "lead_time", "operator": "<=", "value": "10 hours"},
   >>> ]



   ..
       !! processed by numpydoc !!

.. py:function:: create_join_and_save_timeseries_query(jtq: teehr.models.queries_database.JoinedTimeseriesQuery) -> str

   
   Load joined timeseries into a duckdb persistent database using a
   database query.


   :Parameters:

       **jtq** : JoinedTimeseriesQuery
           Pydantic model containing query parameters.

   :Returns:

       str
           The query string.













   ..
       !! processed by numpydoc !!

.. py:function:: describe_timeseries(timeseries_filepath: str) -> Dict

   
   Retrieve descriptive stats for a time series.


   :Parameters:

       **timeseries_filepath** : str
           File path to the "observed" data.  String must include path to file(s)
           and can include wildcards.  For example, "/path/to/parquet/\*.parquet".

   :Returns:

       Dict
           A dictionary of summary statistics for a timeseries.













   ..
       !! processed by numpydoc !!

.. py:function:: create_get_joined_timeseries_query(jtq: teehr.models.queries_database.JoinedTimeseriesQuery) -> str

   
   Retrieve joined timeseries using database query.


   :Parameters:

       **jtq** : JoinedTimeseriesQuery
           Pydantic model containing query parameters.

   :Returns:

       str
           The query string.








   .. rubric:: Notes

   Filter By Fields:

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name
   * lead_time
   * absolute_difference
   * [any user-added fields]

   Order By Fields:

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name


   .. rubric:: Examples

   >>> order_by = ["lead_time", "primary_location_id"]

   >>> group_by = ["lead_time", "primary_location_id"]

   >>> filters = [
   >>>     {
   >>>         "column": "primary_location_id",
   >>>         "operator": "=",
   >>>         "value": "gage-A",
   >>>     },
   >>>     {
   >>>         "column": "reference_time",
   >>>         "operator": "=",
   >>>         "value": "2022-01-01 00:00:00",
   >>>     },
   >>>     {"column": "lead_time", "operator": "<=", "value": "10 hours"},
   >>> ]



   ..
       !! processed by numpydoc !!

.. py:function:: create_get_timeseries_query(tq: teehr.models.queries_database.TimeseriesQuery) -> str

   
   Retrieve joined timeseries using database query.


   :Parameters:

       **tq** : TimeseriesQuery
           Pydantic model containing query parameters.

   :Returns:

       str
           The query string.








   .. rubric:: Notes

   Filter By Fields:

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name
   * lead_time
   * absolute_difference
   * [any user-added fields]

   Order By Fields:

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name


   .. rubric:: Examples

   >>> order_by = ["lead_time", "primary_location_id"]

   >>> group_by = ["lead_time", "primary_location_id"]

   >>> filters = [
   >>>     {
   >>>         "column": "primary_location_id",
   >>>         "operator": "=",
   >>>         "value": "gage-A",
   >>>     },
   >>>     {
   >>>         "column": "reference_time",
   >>>         "operator": "=",
   >>>         "value": "2022-01-01 00:00:00",
   >>>     },
   >>>     {"column": "lead_time", "operator": "<=", "value": "10 hours"},
   >>> ]



   ..
       !! processed by numpydoc !!

.. py:function:: create_get_timeseries_char_query(tcq: teehr.models.queries_database.TimeseriesCharQuery) -> str

   
   Retrieve joined timeseries using database query.


   :Parameters:

       **tcq** : TimeseriesCharQuery
           Pydantic model containing query parameters.

   :Returns:

       str
           The query string.








   .. rubric:: Notes

   Filter, Order By and Group By Fields

   * reference_time
   * primary_location_id
   * secondary_location_id
   * primary_value
   * secondary_value
   * value_time
   * configuration
   * measurement_unit
   * variable_name
   * lead_time
   * [any user-added fields]


   .. rubric:: Examples

   >>> order_by = ["lead_time", "primary_location_id"]

   >>> group_by = ["lead_time", "primary_location_id"]

   >>> filters = [
   >>>     {
   >>>         "column": "primary_location_id",
   >>>         "operator": "=",
   >>>         "value": "gage-A",
   >>>     },
   >>>     {
   >>>         "column": "reference_time",
   >>>         "operator": "=",
   >>>         "value": "2022-01-01 00:00:00",
   >>>     },
   >>>     {"column": "lead_time", "operator": "<=", "value": "10 hours"},
   >>> ]



   ..
       !! processed by numpydoc !!

.. py:function:: create_unique_field_values_query(fn: teehr.models.queries_database.JoinedTimeseriesFieldName) -> str

   
   Create a query for identifying unique values in a field.


   :Parameters:

       **fn** : JoinedTimeseriesFieldName
           Name of the field to query for unique values.

   :Returns:

       str
           The query string.













   ..
       !! processed by numpydoc !!

