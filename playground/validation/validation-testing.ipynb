{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Iterable\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_df(path: Union[Path, str]):\n",
    "    df = duckdb.query(f\"\"\"\n",
    "        SELECT name, type, converted_type, logical_type\n",
    "        FROM parquet_schema('{path}')\n",
    "        WHERE repetition_type = 'OPTIONAL'\n",
    "        ;\n",
    "        \"\"\").df()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df: pd.DataFrame, schema: dict):\n",
    "    valid = True\n",
    "    messages = []\n",
    "    if len(df) !=  len(schema):\n",
    "        # print(f\"schema should have {len(validation_data)} fileds but has {len(df)}\")\n",
    "        messages.append(f\"schema should have {len(validation_data)} fields but has {len(df)}.\")\n",
    "        valid = False\n",
    "    \n",
    "    for vd in schema:\n",
    "        # get row by name\n",
    "        row = df[df['name'] == vd['name']]\n",
    "        if len(row) < 1:\n",
    "            # print(f\"No field named {vd['name']}\")\n",
    "            messages.append(f\"No field named {vd['name']}.\")\n",
    "            valid = False\n",
    "        if len(row) > 1:\n",
    "            # print(f\"More than one field named {vd['name']}\")\n",
    "            messages.append(f\"More than one field named {vd['name']}.\")\n",
    "            valid = False\n",
    "            \n",
    "        rec = row.to_dict(orient='records')[0]\n",
    "        \n",
    "        if not rec['type'] == vd['type']:\n",
    "            # print(f\"{vd['name']} has type {rec['type']} but should be {vd['type']}\")\n",
    "            messages.append(f\"{vd['name']} has type {rec['type']} but should be {vd['type']}.\")\n",
    "            valid = False\n",
    "    \n",
    "        if not rec['converted_type'] == vd['converted_type']:\n",
    "            # print(f\"{vd['name']} has type {rec['converted_type']} but should be {vd['converted_type']}\")\n",
    "            messages.append(f\"{vd['name']} has converted_type {rec['converted_type']} but should be {vd['converted_type']}.\")\n",
    "            valid = False\n",
    "        \n",
    "    return valid, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_path(path: Union[Path, str], pattern: str, schema: dict):\n",
    "    path = Path(path)\n",
    "    if path.is_dir():\n",
    "        files = Path(path).glob(pattern)\n",
    "    else:\n",
    "        files = [path]\n",
    "        \n",
    "    for file in files:\n",
    "        df = get_schema_df(file)\n",
    "        passed, messages = validate_schema(df, schema)\n",
    "        if not passed:\n",
    "            raise ValueError(f\"File: {str(file)} not valid.\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt_cast_to(path: Union[str, Path], backup_file: bool = True, cleanup: bool = False):\n",
    "    \"\"\"Attempt to fix parquet files by casting columns to correct type.\n",
    "\n",
    "    Will only work locally where user has write permissions.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    backup_path = Path(f\"{path}.bak\")\n",
    "    if path.is_file():\n",
    "        # operation could be destructive. back up file\n",
    "        if backup_file:    \n",
    "            shutil.copy(path, backup_path)\n",
    "\n",
    "        # try to fix it\n",
    "        duckdb.query(f\"\"\"\n",
    "            COPY (\n",
    "                SELECT \n",
    "                    location_id::varchar as location_id\n",
    "                    , value_time::timestamp as value_time\n",
    "                    , reference_time::timestamp as reference_time\n",
    "                    , value::float as value\n",
    "                    , variable_name::varchar as variable_name\n",
    "                    , measurement_unit::varchar as measurement_unit\n",
    "                    , configuration::varchar as configuration\n",
    "                FROM read_parquet('{path}')\n",
    "            ) TO '{path}' (FORMAT PARQUET);\n",
    "        \"\"\")\n",
    "\n",
    "        # check that len(org) == len(fixed) and clean up\n",
    "        if cleanup:\n",
    "            should_cleanup = True\n",
    "            df = duckdb.query(f\"\"\"\n",
    "                SELECT\n",
    "                    (SELECT count(*) FROM read_parquet('{path}')) as new\n",
    "                    , (SELECT count(*) FROM read_parquet('{backup_path}')) as org\n",
    "            ;\"\"\").to_df()\n",
    "\n",
    "            if not (df[\"new\"][0] == df[\"org\"][0]):\n",
    "                should_cleanup = False\n",
    "\n",
    "            df = duckdb.query(f\"\"\"\n",
    "                SELECT\n",
    "                    (SELECT count(distinct(location_id)) FROM read_parquet('{path}')) as new\n",
    "                    , (SELECT count(distinct(location_id)) FROM read_parquet('{backup_path}')) as org\n",
    "            ;\"\"\").to_df()\n",
    "\n",
    "            if not (df[\"new\"][0] == df[\"org\"][0]):\n",
    "                should_cleanup = False\n",
    "\n",
    "\n",
    "            if should_cleanup:\n",
    "                backup_path.unlink()\n",
    "    else:\n",
    "        raise FileNotFoundError(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_path_schema(path: Union[Path, str], pattern: str, backup_file: bool = True, cleanup: bool = False):\n",
    "    path = Path(path)\n",
    "    if path.is_dir():\n",
    "        files = Path(path).glob(pattern)\n",
    "    else:\n",
    "        files = [path]\n",
    "        \n",
    "    for file in files:\n",
    "        attempt_cast_to(file, backup_file=backup_file, cleanup=cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesLocalPath():\n",
    "    \n",
    "    def __init__(self, path: Union[Path, str, List[Union[Path, str]]], pattern: str = '**/*.parquet'):\n",
    "        self.path = path\n",
    "        self.pattern = pattern\n",
    "        self.path_patterns = self._get_path_patterns()\n",
    "\n",
    "        self.timeseries_schema = [\n",
    "            {\n",
    "                \"name\": \"location_id\",\n",
    "                \"type\": \"BYTE_ARRAY\",\n",
    "                \"converted_type\": \"UTF8\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"reference_time\",\n",
    "                \"type\": \"INT64\",\n",
    "                \"converted_type\": \"TIMESTAMP_MICROS\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"value_time\",\n",
    "                \"type\": \"INT64\",\n",
    "                \"converted_type\": \"TIMESTAMP_MICROS\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"value\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"converted_type\": None\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"variable_name\",\n",
    "                \"type\": \"BYTE_ARRAY\",\n",
    "                \"converted_type\": \"UTF8\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"measurement_unit\",\n",
    "                \"type\": \"BYTE_ARRAY\",\n",
    "                \"converted_type\": \"UTF8\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"configuration\",\n",
    "                \"type\": \"BYTE_ARRAY\",\n",
    "                \"converted_type\": \"UTF8\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def _get_path_patterns(self):\n",
    "        if isinstance(self.path, List):\n",
    "            strs = []\n",
    "            for p in self.path:\n",
    "                p = Path(p)\n",
    "                if p.is_dir():\n",
    "                    strs.append(str(Path(p, self.pattern)))\n",
    "                else:\n",
    "                    strs.append(str(Path(p)))\n",
    "            return f\"{[s for s in strs]}\"\n",
    "        else:\n",
    "            p = Path(self.path)\n",
    "            if p.is_dir():\n",
    "                return str(Path(p, self.pattern))\n",
    "            else:\n",
    "                return str(Path(p))\n",
    "        \n",
    "    def validate(self):\n",
    "        if isinstance(self.path, List):\n",
    "            for p in self.path:\n",
    "                validate_path(p, self.pattern, self.timeseries_schema)\n",
    "        else:\n",
    "            validate_path(self.path, self.pattern, self.timeseries_schema)\n",
    "\n",
    "    def fix_schema(self, backup_file: bool = True, cleanup: bool = False): \n",
    "        if isinstance(self.path, List):\n",
    "            for p in self.path:\n",
    "                fix_path_schema(p, self.pattern, backup_file=backup_file, cleanup=cleanup)\n",
    "        else:\n",
    "            fix_path_schema(self.path, self.pattern, backup_file=backup_file, cleanup=cleanup)\n",
    "            \n",
    "    def __str__(self):\n",
    "        return self.path_patterns\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"TimeseriesPath(path: {self.path}, pattern: {self.pattern})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeseriesLocalPath('/data/common/baselines/nwm30_retrospective_conus/streamflow_hourly_inst/')\n",
    "print(ts)\n",
    "ts.fix_schema(cleanup=True)\n",
    "ts.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.query(\"\"\"\n",
    "SELECT distinct(variable_name)\n",
    "FROM read_parquet('/data/playground/retro_demo/retro/timeseries/usgs_2016.parquet');\"\"\").df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(variable_name_domains).intersection(set(df[\"variable_name\"]))) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.query(\"\"\"\n",
    "SELECT distinct(measurement_unit)\n",
    "FROM read_parquet('/data/playground/retro_demo/retro/timeseries/usgs_2016.parquet');\"\"\").df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_unit_domains = [\"cms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(measurement_unit_domains).intersection(set(df[\"measurement_unit\"]))) >= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
