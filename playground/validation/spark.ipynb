{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampNTZType, FloatType, IntegerType\n",
    "\n",
    "\n",
    "from teehr import Evaluation\n",
    "from pathlib import Path\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a path to the directory where the evaluation will be created\n",
    "TEST_STUDY_DIR = Path(Path().home(), \"temp\", \"test_study\")\n",
    "shutil.rmtree(TEST_STUDY_DIR, ignore_errors=True)\n",
    "TEST_STUDY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set a path to the directory where the test data is stored\n",
    "TEST_DATA_DIR = Path(\"/home/matt/repos/teehr/tests/data/v0_3_test_study\")\n",
    "GEOJSON_GAGES_FILEPATH = Path(TEST_DATA_DIR, \"geo\", \"gages.geojson\")\n",
    "PRIMARY_TIMESERIES_FILEPATH = Path(\n",
    "    TEST_DATA_DIR, \"timeseries\", \"test_short_obs.parquet\"\n",
    ")\n",
    "CROSSWALK_FILEPATH = Path(TEST_DATA_DIR, \"geo\", \"crosswalk.csv\")\n",
    "SECONDARY_TIMESERIES_FILEPATH = Path(\n",
    "    TEST_DATA_DIR, \"timeseries\", \"test_short_fcast.parquet\"\n",
    ")\n",
    "GEO_FILEPATH = Path(TEST_DATA_DIR, \"geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Evaluation object\n",
    "eval = Evaluation(dir_path=TEST_STUDY_DIR)\n",
    "\n",
    "# Enable logging\n",
    "eval.enable_logging()\n",
    "\n",
    "# Clone the template\n",
    "eval.clone_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(PRIMARY_TIMESERIES_FILEPATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('reference_time', TimestampNTZType(), True),\n",
    "        StructField('value_time', TimestampNTZType(), True),\n",
    "        StructField('value', DoubleType(), True),\n",
    "        StructField('variable_name', StringType(), True),\n",
    "        StructField('configuration_name', StringType(), True),\n",
    "        StructField('unit_name', StringType(), True),\n",
    "        StructField('location_id', StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = (\n",
    "    eval.spark.read.format(\"parquet\")\n",
    "    # .schema(schema)\n",
    "    .load(str(PRIMARY_TIMESERIES_FILEPATH))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current: desired\n",
    "# https://datamadness.medium.com/renaming-columns-in-pyspark-fe71f7111454\n",
    "rename_dict = {\n",
    "  'measurement_unit':'unit_name',\n",
    "  'configuration':'configuration_name',\n",
    "}\n",
    "\n",
    "timeseries = (\n",
    "  timeseries\n",
    "  .select([col(c).alias(rename_dict.get(c, c)) for c in timeseries.columns])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datamadness.medium.com/casting-data-types-in-pyspark-f95d1326449b\n",
    "\n",
    "# data_type_map = {\n",
    "#   'reference_time': TimestampNTZType(),\n",
    "#   'value_time': TimestampNTZType(),\n",
    "#   'value': DoubleType(),\n",
    "#   'variable_name': StringType(),\n",
    "#   'unit_name': DoubleType(),\n",
    "#   'configuration_name': StringType(),\n",
    "#   'location_id': StringType()\n",
    "# }\n",
    "\n",
    "\n",
    "# timeseries = (\n",
    "#     timeseries\n",
    "#   .select([col(column_schema[0]).cast(data_type_map.get(column_schema[0], column_schema[1])) for column_schema in timeseries.dtypes])\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    timeseries.write\n",
    "    .partitionBy(\"configuration_name\", \"variable_name\")\n",
    "    .format(\"parquet\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(str(Path(eval.primary_timeseries_dir)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "# schema = pa.DataFrameSchema({\n",
    "#     \"reference_time\": pa.Column(str),\n",
    "#     \"value_time\": pa.Column(str),\n",
    "#     \"value\": pa.Column(float),\n",
    "#     \"variable_name\": pa.Column(str),\n",
    "#     \"configuration_name\": pa.Column(str),\n",
    "#     \"unit_name\": pa.Column(str),\n",
    "#     \"location_id\": pa.Column(str)\n",
    "# })\n",
    "\n",
    "# validated_df = schema(timeseries)\n",
    "# print(validated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera.pyspark as pa\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from decimal import Decimal\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pandera.pyspark import DataFrameModel\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "class PanderaSchema(DataFrameModel):\n",
    "    id: T.IntegerType() = pa.Field(gt=5)\n",
    "    product_name: T.StringType() = pa.Field(str_startswith=\"B\")\n",
    "    price: T.DecimalType(20, 5) = pa.Field()\n",
    "    description: T.ArrayType(T.StringType()) = pa.Field()\n",
    "    meta: T.MapType(T.StringType(), T.StringType()) = pa.Field()\n",
    "\n",
    "data = [\n",
    "    (5, \"Bread\", Decimal(44.4), [\"description of product\"], {\"product_category\": \"dairy\"}),\n",
    "    (15, \"Butter\", Decimal(99.0), [\"more details here\"], {\"product_category\": \"bakery\"}),\n",
    "]\n",
    "\n",
    "spark_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"id\", T.IntegerType(), False),\n",
    "        T.StructField(\"product\", T.StringType(), False),\n",
    "        T.StructField(\"price\", T.DecimalType(20, 5), False),\n",
    "        T.StructField(\"description\", T.ArrayType(T.StringType(), False), False),\n",
    "        T.StructField(\n",
    "            \"meta\", T.MapType(T.StringType(), T.StringType(), False), False\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "df = spark.createDataFrame(data, spark_schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = PanderaSchema.validate(check_obj=df)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanderaTimeseriesSchema(DataFrameModel):\n",
    "    reference_time: T.TimestampType = pa.Field(nullable=True)\n",
    "    value_time: T.TimestampType = pa.Field()\n",
    "    value: T.DoubleType = pa.Field()\n",
    "    variable_name: T.StringType = pa.Field()\n",
    "    configuration_name: T.StringType = pa.Field()\n",
    "    unit_name: T.StringType = pa.Field(isin=[\"m^3/s\"])\n",
    "    location_id: T.StringType = pa.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_out = PanderaTimeseriesSchema.validate(check_obj=timeseries)\n",
    "ts_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_out_errors = ts_out.pandera.errors\n",
    "print(json.dumps(dict(df_out_errors), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"reference_time\": pa.Column(T.TimestampNTZType, nullable=True),\n",
    "    \"value_time\": pa.Column(T.TimestampNTZType),\n",
    "    \"value\": pa.Column(T.FloatType, coerce=True),\n",
    "    \"variable_name\": pa.Column(T.StringType),\n",
    "    \"configuration_name\": pa.Column(T.StringType),\n",
    "    \"unit_name\": pa.Column(T.StringType, pa.Check.isin([\"m^3/s\"])),\n",
    "    \"location_id\": pa.Column(T.StringType)\n",
    "})\n",
    "\n",
    "validated_df = schema(timeseries)\n",
    "print(validated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_out_errors = validated_df.pandera.errors\n",
    "print(json.dumps(dict(df_out_errors), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(schema.columns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
