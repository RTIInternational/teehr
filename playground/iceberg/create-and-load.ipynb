{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6136814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import teehr\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21918262",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = str(Path.home() / \"temp\" / \"iceberg\" / \"evaluation\")\n",
    "warehouse_path = str(Path(evaluation_path) / \"spark-warehouse\")\n",
    "catalog_name = \"local\"\n",
    "schema_name = \"db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e929cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /Users/mdenno/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mdenno/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4c19cfdc-4ec4-4382-b37b-8ba5ee17077b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.7.1 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.7.1 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.20.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/mdenno/repos/teehr/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.7.1 in central\n",
      "\tfound org.apache.sedona#shade-proto;1.7.1 in central\n",
      "\tfound org.xerial#sqlite-jdbc;3.41.2.2 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.7.1-28.5 in central\n",
      ":: resolution report :: resolve 291ms :: artifacts dl 50ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.0 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.7.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.7.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.7.1 from central in [default]\n",
      "\torg.apache.sedona#shade-proto;1.7.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.7.1-28.5 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.20.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\torg.xerial#sqlite-jdbc;3.41.2.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   28  |   0   |   0   |   2   ||   26  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4c19cfdc-4ec4-4382-b37b-8ba5ee17077b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 26 already retrieved (0kB/14ms)\n",
      "25/06/01 22:24:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "config = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.sedona:sedona-spark-3.5_2.12:1.7.1,\"\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,\"\n",
    "        \"org.datasyslab:geotools-wrapper:1.7.1-28.5\"\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.jars.repositories\",\n",
    "        \"https://artifacts.unidata.ucar.edu/repository/unidata-all\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}\",\n",
    "        \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\"\n",
    "    )\n",
    "    .config(\n",
    "        f\"spark.sql.catalog.{catalog_name}.warehouse\",\n",
    "        f\"{warehouse_path}/{catalog_name}\"\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.driver.host\", \"localhost\")\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sedona = SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d086ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing Evaluation (useful when testing)\n",
    "shutil.rmtree(evaluation_path, ignore_errors=True)\n",
    "\n",
    "# Create an Evaluation object and create the directory\n",
    "ev = teehr.Evaluation(\n",
    "    dir_path=evaluation_path,\n",
    "    create_dir=True,\n",
    "    spark=sedona\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b39ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0_2_location_example</td>\n",
       "      <td>Example evaluation datsets with 2 USGS gages</td>\n",
       "      <td>s3a://ciroh-rti-public-data/teehr-data-warehou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e1_camels_daily_streamflow</td>\n",
       "      <td>Daily average streamflow at ther Camels basins</td>\n",
       "      <td>s3a://ciroh-rti-public-data/teehr-data-warehou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2_camels_hourly_streamflow</td>\n",
       "      <td>Hourly instantaneous streamflow at ther Camels...</td>\n",
       "      <td>s3a://ciroh-rti-public-data/teehr-data-warehou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e3_usgs_hourly_streamflow</td>\n",
       "      <td>Hourly instantaneous streamflow at USGS CONUS ...</td>\n",
       "      <td>s3a://ciroh-rti-public-data/teehr-data-warehou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4_nwm_operational</td>\n",
       "      <td>Empty template to load and evaluate NWM operat...</td>\n",
       "      <td>s3a://ciroh-rti-public-data/teehr-data-warehou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "0        e0_2_location_example   \n",
       "1   e1_camels_daily_streamflow   \n",
       "2  e2_camels_hourly_streamflow   \n",
       "3    e3_usgs_hourly_streamflow   \n",
       "4           e4_nwm_operational   \n",
       "\n",
       "                                         description  \\\n",
       "0       Example evaluation datsets with 2 USGS gages   \n",
       "1     Daily average streamflow at ther Camels basins   \n",
       "2  Hourly instantaneous streamflow at ther Camels...   \n",
       "3  Hourly instantaneous streamflow at USGS CONUS ...   \n",
       "4  Empty template to load and evaluate NWM operat...   \n",
       "\n",
       "                                                 url  \n",
       "0  s3a://ciroh-rti-public-data/teehr-data-warehou...  \n",
       "1  s3a://ciroh-rti-public-data/teehr-data-warehou...  \n",
       "2  s3a://ciroh-rti-public-data/teehr-data-warehou...  \n",
       "3  s3a://ciroh-rti-public-data/teehr-data-warehou...  \n",
       "4  s3a://ciroh-rti-public-data/teehr-data-warehou...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev.list_s3_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e8ff3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/01 22:24:06 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/06/01 22:24:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Clone the e0_camels_daily_streamflow evaluation from the S3 bucket\n",
    "# ev.clone_from_s3(\"e1_camels_daily_streamflow\")\n",
    "ev.clone_from_s3(\"e0_2_location_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edd3afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying schema version 1 to local.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying schema version 2 to local.db\n",
      "Applying schema version 3 to local.db\n",
      "Applying schema version 4 to local.db\n",
      "Schema evolution completed for local.\n"
     ]
    }
   ],
   "source": [
    "import apply_migrations\n",
    "import importlib\n",
    "importlib.reload(apply_migrations)\n",
    "\n",
    "apply_migrations.evolve_catalog_schema(sedona, catalog_name, schema_name)\n",
    "\n",
    "print(f\"Schema evolution completed for {catalog_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f013f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_sdf = ev.units.to_sdf()\n",
    "units_sdf.writeTo(\"local.db.units\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ada91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_sdf = ev.configurations.to_sdf()\n",
    "configuration_sdf.writeTo(\"local.db.configurations\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec5bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_sdf = ev.variables.to_sdf()\n",
    "variables_sdf.writeTo(\"local.db.variables\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "561da1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_sdf = ev.attributes.to_sdf()\n",
    "attributes_sdf.writeTo(\"local.db.attributes\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a0af6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_sdf = ev.locations.to_sdf()\n",
    "locations_sdf.writeTo(\"local.db.locations\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a98df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+\n",
      "|  location_id|     attribute_name|               value|\n",
      "+-------------+-------------------+--------------------+\n",
      "|usgs-14316700|            aridity|   0.501305088892464|\n",
      "|usgs-14138800|            aridity|   0.270945979187767|\n",
      "|usgs-14316700|     dom_land_cover|    Evergreen Nee...|\n",
      "|usgs-14138800|     dom_land_cover|    Evergreen Nee...|\n",
      "|usgs-14316700|dom_land_cover_frac|   0.999999999999874|\n",
      "|usgs-14138800|dom_land_cover_frac|                 1.0|\n",
      "|usgs-14316700|      drainage_area|               587.9|\n",
      "|usgs-14138800|      drainage_area|                21.2|\n",
      "|usgs-14316700|          elev_mean|              952.26|\n",
      "|usgs-14138800|          elev_mean|              821.62|\n",
      "|usgs-14316700|        forest_frac|                 1.0|\n",
      "|usgs-14138800|        forest_frac|                 1.0|\n",
      "|usgs-14316700|          frac_snow|   0.176336580742005|\n",
      "|usgs-14138800|          frac_snow|   0.317266212149897|\n",
      "|usgs-14316700|         frac_urban|                 0.0|\n",
      "|usgs-14138800|         frac_urban|                 0.0|\n",
      "|usgs-14316700|     high_prec_freq|               14.75|\n",
      "|usgs-14138800|     high_prec_freq|               12.55|\n",
      "|usgs-14316700|             p_mean|    4.54340041067762|\n",
      "|usgs-14138800|             p_mean|    7.72975085557837|\n",
      "+-------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_attrs_sdf = ev.location_attributes.to_sdf()\n",
    "location_attrs_sdf.writeTo(\"local.db.location_attributes\").append()\n",
    "location_attrs_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af27917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+---------+---------+-------------+------------------+--------------------+\n",
      "|reference_time|         value_time|    value|unit_name|  location_id|configuration_name|       variable_name|\n",
      "+--------------+-------------------+---------+---------+-------------+------------------+--------------------+\n",
      "|          NULL|2000-10-01 00:00:00|3.3413877|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 01:00:00|3.9926753|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 02:00:00| 4.445745|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 03:00:00| 5.408518|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 04:00:00|5.6067357|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 05:00:00| 5.153666|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 06:00:00|4.5590124|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 07:00:00|5.2952504|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 08:00:00| 7.730499|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 09:00:00| 9.825946|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 10:00:00|10.562183|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 11:00:00|10.788718|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 12:00:00|10.165748|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 13:00:00| 9.769312|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 14:00:00|  8.89149|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 15:00:00| 8.381786|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 16:00:00| 9.995847|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 17:00:00|11.015253|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 18:00:00| 9.995847|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "|          NULL|2000-10-01 19:00:00|  8.97644|    m^3/s|usgs-14138800| usgs_observations|streamflow_hourly...|\n",
      "+--------------+-------------------+---------+---------+-------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_attrs_sdf = ev.primary_timeseries.to_sdf()\n",
    "location_attrs_sdf.writeTo(\"local.db.primary_timeseries\").append()\n",
    "location_attrs_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb68e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+--------------+------+-------------------+--------------------+--------------+\n",
      "|         value_time|value|unit_name|   location_id|member| configuration_name|       variable_name|reference_time|\n",
      "+-------------------+-----+---------+--------------+------+-------------------+--------------------+--------------+\n",
      "|2000-10-01 00:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 00:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 01:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 01:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 02:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 02:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 03:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 03:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 04:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 04:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 05:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 05:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 06:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 06:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 07:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 07:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 08:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 08:00:00| 0.06|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 09:00:00| 0.38|    m^3/s|nwm30-23894572|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "|2000-10-01 09:00:00| 0.07|    m^3/s|nwm30-23736071|  NULL|nwm30_retrospective|streamflow_hourly...|          NULL|\n",
      "+-------------------+-----+---------+--------------+------+-------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "secondary_timeseries_sdf = ev.secondary_timeseries.to_sdf()\n",
    "secondary_timeseries_sdf.writeTo(\"local.db.secondary_timeseries\").append()\n",
    "secondary_timeseries_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91340bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+\n",
      "|primary_location_id|secondary_location_id|\n",
      "+-------------------+---------------------+\n",
      "|      usgs-14316700|       nwm30-23894572|\n",
      "|      usgs-14138800|       nwm30-23736071|\n",
      "+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_crosswalk_sdf = ev.location_crosswalks.to_sdf()\n",
    "location_crosswalk_sdf.writeTo(\"local.db.location_crosswalks\").append()\n",
    "location_crosswalk_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3470218",
   "metadata": {},
   "outputs": [],
   "source": [
    "sedona.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc543c2",
   "metadata": {},
   "source": [
    "- Create joined timeseries needs some filters so that we can incrementally create the joined timeseries.  \n",
    "- Joined timeseries should maybe only contain the joined ts.  That join process does not parallelize well.  Other joins do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2323ecf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
