{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arch.bootstrap import StationaryBootstrap, CircularBlockBootstrap\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pull out a single timeseries from the joined parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kling_gupta_efficiency(p: pd.Series, s: pd.Series) -> float:\n",
    "\n",
    "    if len(s) == 0 or len(s) == 0:\n",
    "        return np.nan\n",
    "    std_p = np.std(p)\n",
    "    mean_p = np.mean(p)\n",
    "    std_s = np.std(s)\n",
    "\n",
    "    if std_p == 0 or mean_p == 0 or std_s == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Pearson correlation coefficient\n",
    "    linear_correlation = np.corrcoef(s, p)[0,1]\n",
    "\n",
    "    # Relative variability\n",
    "    relative_variability = std_s / std_p\n",
    "\n",
    "    # Relative mean\n",
    "    relative_mean = np.mean(s) / mean_p\n",
    "\n",
    "    # Scaled Euclidean distance\n",
    "    euclidean_distance = np.sqrt(\n",
    "        (1 * (linear_correlation - 1.0)) ** 2.0 +\n",
    "        (1 * (relative_variability - 1.0)) ** 2.0 +\n",
    "        (1* (relative_mean - 1.0)) ** 2.0\n",
    "        )\n",
    "\n",
    "    # Return KGE\n",
    "    return 1.0 - euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName('TEEHR')\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "    .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    ")\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "## See spark config if ya want\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType, StringType, MapType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf( MapType(StringType(), FloatType()) )\n",
    "def bs_kling_gupta_efficiency(p: pd.Series, s: pd.Series) -> float:\n",
    "\n",
    "    bs = CircularBlockBootstrap(365, p, s, seed=1234)\n",
    "    results = bs.apply(kling_gupta_efficiency, 1000)\n",
    "    quantiles = (0.05, 0.50, 0.95)\n",
    "    values = np.quantile(results, quantiles)\n",
    "    quantiles = [f\"KGE_{str(i)}\" for i in quantiles]\n",
    "    d = dict(zip(quantiles,values))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"bs_kling_gupta_efficiency\", bs_kling_gupta_efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/protocols/p1_daily_streamflow_sim/teehr_database/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "spark.catalog.dropTempView(\"joined_temp\")\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , bs_kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as bs_kling_gupta_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "   --WHERE primary_location_id IN ('usgs-01010070', 'usgs-01105500')\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.functions as F\n",
    "# keys_df = sdf.select(F.explode(F.map_keys(F.col(\"bs_kling_gupta_efficiency\")))).distinct()\n",
    "# keys = list(map(lambda row: row[0], keys_df.collect()))\n",
    "# key_cols = list(map(lambda f: F.col(\"bs_kling_gupta_efficiency\").getItem(f).alias(str(f)), keys))\n",
    "# sdf.select(key_cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(map(\n",
    "    lambda f: F.col(\"bs_kling_gupta_efficiency\").getItem(f).alias(str(f)),\n",
    "    [\"KGE_0.05\", \"KGE_0.5\", \"KGE_0.95\"]))\n",
    "sdf.select(cols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf( MapType(StringType(), FloatType()) )\n",
    "def bs_kling_gupta_efficiency(p: pd.Series, s: pd.Series) -> float:\n",
    "\n",
    "    bs = CircularBlockBootstrap(365*24, p, s, seed=1234)\n",
    "    results = bs.apply(kling_gupta_efficiency, 1000)\n",
    "    quantiles = (0.05, 0.50, 0.95)\n",
    "    values = np.quantile(results, quantiles)\n",
    "    quantiles = [f\"KGE_{str(i)}\" for i in quantiles]\n",
    "    d = dict(zip(quantiles,values))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/protocols/p2_hourly_streamflow_sim/teehr_database/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "spark.catalog.dropTempView(\"joined_temp\")\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , bs_kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as bs_kling_gupta_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "   --WHERE primary_location_id IN ('usgs-01010070', 'usgs-01105500')\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/playground/mdenno/40-yr-retrospective/dataset/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "spark.catalog.dropTempView(\"joined_temp\")\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , bs_kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as bs_kling_gupta_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "   WHERE primary_location_id IN ('usgs-01010070', 'usgs-01105500')\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , bs_kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as bs_kling_gupta_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "   --WHERE primary_location_id IN ('usgs-01010070', 'usgs-01105500')\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
