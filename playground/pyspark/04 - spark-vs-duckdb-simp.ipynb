{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Spark vs. DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "from teehr.classes.duckdb_joined_parquet import DuckDBJoinedParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.builder.master(\"local[*]\").getOrCreate().stop()\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .setAppName('TEEHR')\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "    .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    ")\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_udfs_complete import nash_sutcliffe_efficiency\n",
    "spark.udf.register(\"nash_sutcliffe_efficiency\", nash_sutcliffe_efficiency)\n",
    "\n",
    "from pandas_udfs_complete import kling_gupta_efficiency\n",
    "spark.udf.register(\"kling_gupta_efficiency\", kling_gupta_efficiency)\n",
    "\n",
    "from pandas_udfs_complete import relative_bias\n",
    "spark.udf.register(\"relative_bias\", relative_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/playground/mdenno/40-yr-retrospective/dataset/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , relative_bias(joined.primary_value, joined.secondary_value) as relative_bias\n",
    "        , kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as kling_gupta_efficiency\n",
    "        , nash_sutcliffe_efficiency(joined.primary_value, joined.secondary_value) as nash_sutcliffe_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "   --WHERE primary_location_id IN ('usgs-01010070', 'usgs-01105500')\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = DuckDBJoinedParquet(f\"{JOINED_TABLE}/**/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = jp.get_metrics(\n",
    "    group_by=[\"primary_location_id\", \"configuration\"],\n",
    "    order_by=[\"primary_location_id\", \"configuration\"],\n",
    "    include_metrics=[\"relative_bias\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "qry = jp.get_metrics(\n",
    "    group_by=[\"primary_location_id\", \"configuration\"],\n",
    "    order_by=[\"primary_location_id\", \"configuration\"],\n",
    "    include_metrics=[\"relative_bias\"],\n",
    "    return_query=True\n",
    ")\n",
    "print(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "            SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    "        )\n",
    "        , metrics AS (\n",
    "            SELECT\n",
    "                joined.primary_location_id,joined.configuration\n",
    "                , sum(secondary_value - primary_value) / sum(primary_value) AS relative_bias\n",
    "            FROM\n",
    "                joined\n",
    "            GROUP BY\n",
    "                joined.primary_location_id,joined.configuration\n",
    "        )\n",
    "        SELECT\n",
    "            metrics.primary_location_id,metrics.configuration\n",
    "            , relative_bias\n",
    "        FROM metrics\n",
    "        ORDER BY\n",
    "            metrics.primary_location_id,metrics.configuration\n",
    "    ;\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "            SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    "        )\n",
    "        ,nse AS (\n",
    "            SELECT\n",
    "                primary_location_id,configuration\n",
    "                ,avg(primary_value) AS avg_primary_value\n",
    "            FROM\n",
    "                joined\n",
    "            GROUP BY\n",
    "                primary_location_id,configuration\n",
    "        )\n",
    "        , metrics AS (\n",
    "            SELECT\n",
    "                joined.primary_location_id,joined.configuration\n",
    "                , 1 - (\n",
    "            sum(pow(joined.primary_value - joined.secondary_value, 2))\n",
    "            / sum(pow(joined.primary_value - nse.avg_primary_value, 2))\n",
    "        ) as nash_sutcliffe_efficiency\n",
    "                , 1 - sqrt(\n",
    "            pow(corr(secondary_value, primary_value) - 1, 2)\n",
    "            + pow(stddev(secondary_value) / stddev(primary_value) - 1, 2)\n",
    "            + pow(avg(secondary_value) / avg(primary_value) - 1, 2)\n",
    "        ) as kling_gupta_efficiency\n",
    "            FROM\n",
    "                joined\n",
    "        INNER JOIN nse\n",
    "        ON nse.primary_location_id = joined.primary_location_id AND nse.configuration = joined.configuration\n",
    "            GROUP BY\n",
    "                joined.primary_location_id,joined.configuration\n",
    "        )\n",
    "        SELECT\n",
    "            metrics.primary_location_id,metrics.configuration\n",
    "            , kling_gupta_efficiency,nash_sutcliffe_efficiency\n",
    "        FROM metrics\n",
    "        ORDER BY\n",
    "            metrics.primary_location_id,metrics.configuration\n",
    "    ;\n",
    "\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/protocols/p1_daily_streamflow_sim/teehr_database/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "spark.catalog.dropTempView(\"joined_temp\")\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as kling_gupta_efficiency\n",
    "        , nash_sutcliffe_efficiency(joined.primary_value, joined.secondary_value) as nash_sutcliffe_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = DuckDBJoinedParquet(f\"{JOINED_TABLE}/**/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = jp.get_metrics(\n",
    "    group_by=[\"primary_location_id\", \"configuration\"],\n",
    "    order_by=[\"primary_location_id\", \"configuration\"],\n",
    "    include_metrics=[\"kling_gupta_efficiency\", \"nash_sutcliffe_efficiency\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_TABLE = \"/data/protocols/p2_hourly_streamflow_sim/teehr_database/joined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read joined from hive partition folder format\n",
    "joined = spark.read.format(\"parquet\").option(\"recursiveFileLookup\", \"true\").load(JOINED_TABLE)\n",
    "spark.catalog.dropTempView(\"joined_temp\")\n",
    "joined.createTempView(\"joined_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate a few basic metrics for python UFDs\n",
    "sdf = spark.sql(\"\"\"\n",
    "WITH joined as (\n",
    "    SELECT\n",
    "        *\n",
    "    FROM joined_temp jt\n",
    ")\n",
    ", metrics AS (\n",
    "    SELECT\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    "        , kling_gupta_efficiency(joined.primary_value, joined.secondary_value) as kling_gupta_efficiency\n",
    "        , nash_sutcliffe_efficiency(joined.primary_value, joined.secondary_value) as nash_sutcliffe_efficiency\n",
    "    FROM\n",
    "        joined\n",
    "    GROUP BY\n",
    "        joined.primary_location_id\n",
    "        , joined.configuration\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM metrics\n",
    "ORDER BY\n",
    "    metrics.primary_location_id\n",
    "    , metrics.configuration\n",
    "\"\"\")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = DuckDBJoinedParquet(f\"{JOINED_TABLE}/**/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = jp.get_metrics(\n",
    "    group_by=[\"primary_location_id\", \"configuration\"],\n",
    "    order_by=[\"primary_location_id\", \"configuration\"],\n",
    "    include_metrics=[\"kling_gupta_efficiency\", \"nash_sutcliffe_efficiency\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
