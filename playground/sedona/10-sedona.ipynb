{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "SPARK_HOME = pyspark.__path__[0]\n",
    "print(f\"SPARK_HOME is: {SPARK_HOME}\")\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "print(f\"SPARK_VERSION is: {SPARK_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Tell Bokeh to output plots in the notebook\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import SedonaContext, ST_GeomFromWKB, ST_SetSRID, ST_AsEWKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TEEHR\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config('spark.jars.packages', 'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0,org.datasyslab:geotools-wrapper:1.7.0-28.5,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.524')\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all')\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the Evaluation will be created\n",
    "test_eval_dir = Path(Path().home(), \"temp\", \"10_sedona\")\n",
    "test_eval_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing Evaluation (useful when testing)\n",
    "# shutil.rmtree(test_eval_dir, ignore_errors=True)\n",
    "\n",
    "# Create an Evaluation object and create the directory\n",
    "# ev = teehr.Evaluation(dir_path=test_eval_dir, create_dir=True, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the e0_2_location_example evaluation from the S3 bucket\n",
    "# ev.clone_from_s3(\"e0_2_location_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to existing Evaluation\n",
    "ev = teehr.Evaluation(dir_path=test_eval_dir, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab locations from database\n",
    "sdf = ev.locations.to_sdf()\n",
    "sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like meta data has to be read from a single file even if the \"table/partition\" is a folder (wait maybe not)\n",
    "# and the files written by TEEHR v0.4.7 does not contain the geoparquet.metadata\n",
    "sdf_meta = (\n",
    "    ev.spark.read.format(\"geoparquet.metadata\")\n",
    "    .load(\"/Users/mdenno/temp/10_sedona/dataset/locations/part-00000-b29ce52b-7ba9-443f-be44-11d204bec7dc-c000.snappy.parquet\")\n",
    ")\n",
    "sdf_meta.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make locations CRS aware with the SRID with SQL and DatafRame API\n",
    "# choose one\n",
    "# gdf =  sdf.withColumn(\"geometry2\", ST_GeomFromWKB(\"geometry\")).drop(\"geometry\").withColumnRenamed(\"geometry2\", \"geometry\")\n",
    "gdf = ev.sql(\"\"\"\n",
    "SELECT id, name, ST_SetSRID(ST_GeomFromWKB(geometry), 4326) as geometry FROM locations\n",
    "\"\"\")\n",
    "gdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query of temp view shows SRID in geometry\n",
    "gdf.createOrReplaceTempView(\"two_locations\")\n",
    "ev.spark.sql(\"SELECT ST_AsEWKT(geometry) FROM two_locations\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the CRS from this convenient available geoparquet file (could put in a const file  or something)\n",
    "# look up form somewhere?\n",
    "projjson_crs = gpd.read_parquet(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations.parquet\").crs.to_json()\n",
    "# projjson_crs = json.loads(projjson_crs)\n",
    "projjson_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a geoparquet with CRS info\n",
    "two_locations_geoparquet_path = \"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations_geoparquet\"\n",
    "(\n",
    "    gdf\n",
    "        .repartition(1)\n",
    "        .write.format(\"geoparquet\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"geoparquet.crs\", projjson_crs)\n",
    "        .save(two_locations_geoparquet_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now read the parquet file just saved with PySpark\n",
    "gdf = ev.spark.read.format(\"geoparquet\").load(two_locations_geoparquet_path)\n",
    "gdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata from the \"folder\" which I thought was not possible...\n",
    "gdf_meta = ev.spark.read.format(\"geoparquet.metadata\").load(two_locations_geoparquet_path)\n",
    "gdf_meta.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# So can the CRS be read from the metadata and added to the geometry?\n",
    "# This seems like it could be problematic since you could (although we don't save data in multiple CRS)\n",
    "espg_code = json.loads(gdf_meta.toPandas()[\"columns\"].values[0][\"geometry\"][\"crs\"])[\"id\"][\"code\"]\n",
    "espg_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at table using SQL\n",
    "gdf.createOrReplaceTempView(\"two_locations\")\n",
    "\n",
    "# does not show SRID (i.e.), so I think the SRID is only in the metadata but not in the geometry\n",
    "# I think this is also \"known\" to be true or was for shapfiles anyway.\n",
    "ev.spark.sql(\"SELECT ST_AsEWKT(geometry) FROM two_locations\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_gdf = ev.sql(\"\"\"\n",
    "SELECT id, name, ST_SetSRID(ST_GeomFromWKB(geometry), 4326) as geometry FROM locations\n",
    "\"\"\")\n",
    "crs_gdf.select(ST_AsEWKT(\"geometry\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_locations_geoparquet_path = Path(two_locations_geoparquet_path)\n",
    "\n",
    "gpd.read_parquet(list(two_locations_geoparquet_path.glob(\"*.parquet\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(ev.locations.dir)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.spark.read.format(\"geoparquet\").load(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations.parquet\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.spark.read.format(\"geoparquet.metadata\").load(path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_locations_df = ev.spark.read.format(\"geoparquet\").options().load(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations.parquet\")\n",
    "two_locations_df.createOrReplaceTempView(\"two_locations\")\n",
    "\n",
    "# does not show SRID\n",
    "ev.spark.sql(\"SELECT ST_AsEWKT(geometry) FROM two_locations\").show(truncate=False)\n",
    "\n",
    "# shows SRID\n",
    "ev.spark.sql(\"SELECT ST_AsEWKT(ST_SetSrid(geometry, 4326)) FROM two_locations\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_locations_df = two_locations_df.withColumn(\"geometry2\", ST_SetSRID(\"geometry\", 4326))\n",
    "two_locations_df.show()\n",
    "two_locations_df.createOrReplaceTempView(\"two_locations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not show SRID\n",
    "ev.spark.sql(\"SELECT ST_AsEWKT(geometry2) FROM two_locations\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_sdf = ev.spark.sql(\"SELECT id, name, ST_Transform(geometry, 'EPSG:4326', 'EPSG:3857') as geometry FROM two_locations\")\n",
    "proj_sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_sdf.write.format(\"geoparquet\").mode(\"overwrite\").save(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations_proj.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.spark.read.format(\"geoparquet\").load(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations_proj.parquet\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.spark.read.format(\"geoparquet.metadata\").load(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations_proj.parquet\")\n",
    "df.select(\"columns\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations.parquet\")\n",
    "# gdf.to_crs(\"EPSG:3857\", inplace=True)\n",
    "projjson_crs = gdf.crs.to_json()\n",
    "gdf.crs.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(gdf).select(ST_AsEWKT(\"geometry\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(\"/Users/mdenno/repos/teehr/tests/data/two_locations/two_locations_proj.parquet\")\n",
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(gdf).select(ST_AsEWKT(\"geometry\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just trying to run metrics in\n",
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kge = teehr.Metrics.KlingGuptaEfficiency()\n",
    "\n",
    "func_pd = pandas_udf(kge.func, kge.attrs[\"return_type\"])\n",
    "\n",
    "spark.udf.register(\"kling_gupta_efficiency\", func_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ev.sql(\"\"\"\n",
    "    SELECT\n",
    "        primary_location_id\n",
    "        , kling_gupta_efficiency(primary_value, secondary_value) as kling_gupta_efficiency\n",
    "    FROM\n",
    "        joined_timeseries\n",
    "    GROUP BY\n",
    "        primary_location_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf = ev.metrics.query(\n",
    "    group_by=[\"primary_location_id\"],\n",
    "    include_metrics=[kge]\n",
    ").to_sdf()\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we could groupby polygon?\n",
    "# Or work with the hydrofabric geopackage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
