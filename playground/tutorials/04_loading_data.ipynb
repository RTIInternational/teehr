{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "In lessons `02` and `03` we explored what data makes up an Evaluation dataset and how it can be accessed using the Evaluation table sub-classes.  In lesson `02` we cloned a complete dataset from AWS S3 to explore. In this lesson we are going to explore how your data can be added to a new or existing Evaluation dataset.\n",
    "\n",
    "Similar to previous lessons we will start by creating a Evaluation class instance that references a directory where the data will be stored, and then clone the Evalation template as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Define the directory where the Evaluation will be created\n",
    "test_eval_dir = Path(Path().home(), \"temp\", \"04_loading_data\")\n",
    "shutil.rmtree(test_eval_dir, ignore_errors=True)\n",
    "\n",
    "# Create an Evaluation object and create the directory\n",
    "ev = teehr.Evaluation(dir_path=test_eval_dir, create_dir=True)\n",
    "\n",
    "# Clone the template\n",
    "ev.clone_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the data from the example dataset. The data is stored in the `tests/data` folder of the teehr repository. We will copy the data to the Evaluation directory, examine the contents and then load into the Evaluation dataset.  Note these data are for demonstration purposes only.  We have given the data fake location IDs and fake timeserries values for demonstration purposes. We will work with real data in future exercises.\n",
    "\n",
    "First lets download some location data.  We will download the `gages.geojson` file which is a GeoJSON file that contains 3 fictional gages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/geo/gages.geojson \\\n",
    "    -O $HOME/temp/04_loading_data/gages.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets open the `gages.geojson` file with GeoPandas and take a look at the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "location_data_path = Path(test_eval_dir, \"gages.geojson\")\n",
    "gdf = gpd.read_file(location_data_path)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the file contains 3 gages that are each represnted by a point.  Next we will load the location data into the TEEHR dataset using methods on the `locations` table sub-class. Note that this file has the `id`, `name` and `geometry` fields required by TEEHR.  If it didn't, the `load_spatial` method can take a `field_mapping` dictionary that can be used to specify which fields should be mapped to which required field name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.locations.load_spatial(location_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the location data has been loaded into the TEEHR dataset, it can be queried with the `ev.locations` class methods, but more importantly, now we can load timeseries that references the `id` in the locations data.  The `id` in the `locations` table is the primary location ID and is refrenced in the `primary_timeseries` table as `location_id`.\n",
    "\n",
    "First lets query the location data to verify it has been loaded, then look at a map to verify it is in the correct locations, then we will move on loading some timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_gdf = ev.locations.to_geopandas()\n",
    "locations_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table of locations came from the TEEHR dataset and matches what we saw in the source file, so we know it was loaded successfuly.  Now lets look at a plot using the built-in teehr.location_map() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Bokeh to output plots in the notebook\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "locations_gdf.teehr.location_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have locaded the location data, we can load some timeseries to the `primary_timeseries` table.  The primary timeseries represents the timeseries that simulations will be compared to (i.e., the truth).\n",
    "\n",
    "Let's download a test timeseries from the TEEHR respository as a *.CSV file, open and examine the contents with Pandas, and load into the TEEHR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/timeseries/test_short_obs.csv \\\n",
    "    -O $HOME/temp/04_loading_data/test_short_obs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "primary_timeseries_path = Path(test_eval_dir, \"test_short_obs.csv\")\n",
    "primary_timeseries_df = pd.read_csv(primary_timeseries_path)\n",
    "primary_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a few things we need to look at and consider before loading the data into the TEEHR dataset.  First is the field names.  You can see above that while the schema of this dataset is close to the TEEHR schema, it is not exactly correct, specifically `configuration` should be `configuration_name` and `measurement_unit` should be `unit_name`.  To address this situation we can use the `field_mapping` argument to mapp the existing field names to the required field names.  The next thing that needs to be delt with are the field values for fields that reference values in another table, for example a domain table or the location table.  Lets start with `unit_name`.  The `unit_name` values need to reference a value from the `name` field in the `units` table.  Next lets look at what values are in the `uints` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.units.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that `m^3/s` is in the `units` table, so we are all set with values in the `measurement_units` field.  Next lets check the `variables` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.variables.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `streamflow` is not in the `name` field of the `variables` table.  There are two ways to remedy this.  1) we could add the value `streamflow` to the `variables` table or 2) we can use the `constant_field_values` argument to set the `variable_name` to an allowed values. In this case we will go with the latter and set the `variable_name` to `streamflow_hourly_inst`.  Next we need to address the `configuration_name` field.  First lets see what values are in the `configurations` table.  Because the `configurations` table is rather large, we will search it for `test_obs` to see if that value is in there (it isn't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.configurations.filter(\n",
    "    {\n",
    "        \"column\": \"name\",\n",
    "        \"operator\": \"=\",\n",
    "        \"value\": \"test_obs\"\n",
    "    }\n",
    ").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by the empty DataFranme that a configuration named `test_obs` is not in the table.  Again, we have two option in this case, we could use the `constant_field_values` argument to set the `configuration_name` to a value that is in the table (like `usgs_observations`) or we could add a new row to the `configurations` table.  Again we will use the `constant_field_values` argument to set the value to a constant.  When we insert the `secondary_timeseries` below we will add a new row to the `configurations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the timeseries data and map over the fields and set constants\n",
    "ev.primary_timeseries.load_csv(\n",
    "    in_path=primary_timeseries_path,\n",
    "    field_mapping={\n",
    "        \"reference_time\": \"reference_time\",\n",
    "        \"value_time\": \"value_time\",\n",
    "        \"configuration\": \"configuration_name\",\n",
    "        \"measurement_unit\": \"unit_name\",\n",
    "        \"variable_name\": \"variable_name\",\n",
    "        \"value\": \"value\",\n",
    "        \"location_id\": \"location_id\"\n",
    "    },\n",
    "    constant_field_values={\n",
    "        \"unit_name\": \"m^3/s\",\n",
    "        \"variable_name\": \"streamflow_hourly_inst\",\n",
    "        \"configuration_name\": \"usgs_observations\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a query for a single timeseries and make sure the data was loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_timeseries_df = ev.primary_timeseries.filter(\n",
    "    {\n",
    "        \"column\": \"location_id\",\n",
    "        \"operator\": \"=\",\n",
    "        \"value\": \"gage-A\"\n",
    "    }\n",
    ").to_pandas()\n",
    "primary_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_timeseries_df.teehr.timeseries_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets load some simulated data that we want to compare to the observed timeseries (primary timeseries).  The secondary timeseries table is where the \"simulated\" timeseries data are stored.  First we will download some test secondary timeseries from the TEEHR repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/timeseries/test_short_fcast.parquet \\\n",
    "    -O $HOME/temp/04_loading_data/test_short_fcast.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_timeseries_path = Path(test_eval_dir, \"test_short_fcast.parquet\")\n",
    "secondary_timeseries_df = pd.read_parquet(secondary_timeseries_path)\n",
    "secondary_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading `secondary_timeseries`, the same considerations regarding field names and field values apply as those for `primary_timeseries`.  Unlike when we loaded the `primary_timeseries`, this time we will add a new configuration to the `configurations` table.  This is more common that the `configuration_name` for the secondary timeseries that you are loading is not in the `configurations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.configurations.add(\n",
    "    teehr.Configuration(\n",
    "        name=\"test_short\",\n",
    "        type=\"secondary\",\n",
    "        description=\"Test Forecast Configuration\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one other consideration before we can load the `secondary_timeseries`.  There is no expectation in the TEEHR schema that the `location_id` used of the `primary_timeseries` and that used for `secondary_timeseries` match.  Therefore, we need to load a location crosswalk data to the `location_crosswalks` table so that TEEHR knows which `primary_location_id` matches which `secondary_location_id`. For example, in our fake test dataset, the observation data at `location_id` \"gage-A\" relates to forecasts with a `location_id` \"fcst-1\".  \n",
    "\n",
    "We already have a file with this information, so like we did with the other test datasets, we will download the data from the TEEHR repository, open it with Pandas to checkout the contents and load into the TEEHR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/geo/crosswalk.csv \\\n",
    "    -O $HOME/temp/04_loading_data/crosswalk.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_path = Path(test_eval_dir, \"crosswalk.csv\")\n",
    "crosswalk_df = pd.read_csv(crosswalk_path)\n",
    "crosswalk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can see that the crosswalk table already has the field names we need (`primary_location_id` and `secondary_location_id`) so we don't need to use the `field_mapping` argument to pass the loading function a field mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crosswalk data to the crosswalks table\n",
    "ev.location_crosswalks.load_csv(crosswalk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the secondary timeseries data and map over the fields and set constants\n",
    "ev.secondary_timeseries.load_parquet(\n",
    "    in_path=secondary_timeseries_path,\n",
    "    field_mapping={\n",
    "        \"reference_time\": \"reference_time\",\n",
    "        \"value_time\": \"value_time\",\n",
    "        \"configuration\": \"configuration_name\",\n",
    "        \"measurement_unit\": \"unit_name\",\n",
    "        \"variable_name\": \"variable_name\",\n",
    "        \"value\": \"value\",\n",
    "        \"location_id\": \"location_id\"\n",
    "    },\n",
    "    constant_field_values={\n",
    "        \"unit_name\": \"m^3/s\",\n",
    "        \"variable_name\": \"streamflow_hourly_inst\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the secondary timeseries data for a single forecast.\n",
    "secondary_timeseries_df = (\n",
    "    ev.secondary_timeseries\n",
    "    .filter(\n",
    "        {\n",
    "            \"column\": \"reference_time\",\n",
    "            \"operator\": \"=\",\n",
    "            \"value\": \"2022-01-01\"\n",
    "        }\n",
    "    )\n",
    ").to_pandas()\n",
    "secondary_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_timeseries_df.teehr.timeseries_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have added data to the following TEEHR data tables:\n",
    "- locations\n",
    "- configurations\n",
    "- location_crosswalks \n",
    "- primary_timeseries\n",
    "- secondary_timeseries\n",
    "\n",
    "This is the bare minimum needed to conduct an evaluation, but we are going to add one more type of data, `location_attributes`.  Location attributes are that provide some type of extra information for the primary locations.  These are data are anologous to the columns in a geospatial data file \"attribute table\" but are stored in a seperate table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/geo/test_attr_2yr_discharge.csv \\\n",
    "    -O $HOME/temp/04_loading_data/test_attr_2yr_discharge.csv\n",
    "\n",
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/geo/test_attr_drainage_area_km2.csv \\\n",
    "    -O $HOME/temp/04_loading_data/test_attr_drainage_area_km2.csv\n",
    "\n",
    "!wget https://github.com/RTIInternational/teehr/raw/refs/heads/main/tests/data/v0_3_test_study/geo/test_attr_ecoregion.csv \\\n",
    "    -O $HOME/temp/04_loading_data/test_attr_ecoregion.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, lets take a look at the raw data we downloaded from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the 3 attribute table contents\n",
    "attr_2yr_discharge_df = pd.read_csv(Path(test_eval_dir, \"test_attr_2yr_discharge.csv\"))\n",
    "display(attr_2yr_discharge_df)\n",
    "\n",
    "attr_drainage_area_km2_df = pd.read_csv(Path(test_eval_dir, \"test_attr_drainage_area_km2.csv\"))\n",
    "display(attr_drainage_area_km2_df)\n",
    "\n",
    "attr_ecoregion_df = pd.read_csv(Path(test_eval_dir, \"test_attr_ecoregion.csv\"))\n",
    "display(attr_ecoregion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see we have data for three different attributes here, `year_2_discharge`,`drainage_area`, `ecoregion`.  Before we can load this data we need to add the attribute names to the `attributes` table and then we can add the location attribute values to the `location_attributes` table.  Like we did for adding configurations, we can use the `ev.attributes.add()` method to add new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some attributes\n",
    "ev.attributes.add(\n",
    "    [\n",
    "        teehr.Attribute(\n",
    "            name=\"drainage_area\",\n",
    "            type=\"continuous\",\n",
    "            description=\"Drainage area in square kilometers\"\n",
    "        ),\n",
    "        teehr.Attribute(\n",
    "            name=\"ecoregion\",\n",
    "            type=\"categorical\",\n",
    "            description=\"Ecoregion\"\n",
    "        ),\n",
    "        teehr.Attribute(\n",
    "            name=\"year_2_discharge\",\n",
    "            type=\"continuous\",\n",
    "            description=\"2-yr discharge in cubic meters per second\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the location attribute data\n",
    "ev.location_attributes.load_csv(\n",
    "    in_path=test_eval_dir,\n",
    "    field_mapping={\"attribute_value\": \"value\"},\n",
    "    pattern=\"test_attr_*.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the location attribute data\n",
    "ev.location_attributes.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have data in all the tables required to conduct and evaluation.  There is one more step to do before we can start \"slicing and dicing\" the data and looking at evaluation metrics - we need to generate the `joined_timeseries` data table.  The `joined_timeseries` table is essentially a materialized view that joins the primary and secondary timeseries joined together based on `location_id`, `value_time`, `variable_name` and `unit_name`, adds the `location_attributes` and adds any user defined fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $HOME/temp/04_loading_data/scripts/user_defined_fields.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.joined_timeseries.create(execute_udf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.joined_timeseries.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.joined_timeseries.to_sdf().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lesson.  The next lesson will "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
