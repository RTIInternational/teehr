{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains an example of how to build a simple TEEHR dataset.\n",
    "\n",
    "The input data is all CSV and GeoJSON files.  This is intended to be\n",
    "the simplest example of how TEEHR can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Setup database paths\n",
    "from pathlib import Path\n",
    "from teehr.classes.duckdb_database import DuckDBDatabase\n",
    "from teehr.classes.duckdb_joined_parquet import DuckDBJoinedParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FILEPATH = Path(\"../raw\")\n",
    "\n",
    "# define the base TEEHR directory location\n",
    "# TEEHR_BASE = Path(Path.home(), \"teehr/example_1/teehr_base\")\n",
    "TEEHR_BASE = Path(\"../teehr_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders for each type of TEEHR 'table'\n",
    "PRIMARY_FILEPATH = Path(TEEHR_BASE, 'primary')\n",
    "SECONDARY_FILEPATH = Path(TEEHR_BASE, 'secondary')\n",
    "CROSSWALK_FILEPATH = Path(TEEHR_BASE, 'crosswalk')\n",
    "GEOMETRY_FILEPATH = Path(TEEHR_BASE, 'geometry')\n",
    "ATTRIBUTE_FILEPATH = Path(TEEHR_BASE, 'attribute')\n",
    "JOINED_FILEPATH = Path(TEEHR_BASE, 'joined')\n",
    "DB_FILEPATH = Path(TEEHR_BASE, 'teehr.db')\n",
    "\n",
    "PRIMARY_FILEPATH.mkdir(exist_ok=True, parents=True)\n",
    "SECONDARY_FILEPATH.mkdir(exist_ok=True, parents=True)\n",
    "CROSSWALK_FILEPATH.mkdir(exist_ok=True, parents=True)\n",
    "GEOMETRY_FILEPATH.mkdir(exist_ok=True, parents=True)\n",
    "ATTRIBUTE_FILEPATH.mkdir(exist_ok=True, parents=True)\n",
    "JOINED_FILEPATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert location dat to parquet\n",
    "locations = gpd.read_file(Path(RAW_DATA_FILEPATH, \"gages.geojson\"))\n",
    "locations.to_parquet(Path(GEOMETRY_FILEPATH, \"locations.parquet\"))\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert crosswalks\n",
    "sim_xw = pd.read_csv(Path(RAW_DATA_FILEPATH, \"sim-crosswalk.csv\"))\n",
    "sim_xw.to_parquet(Path(CROSSWALK_FILEPATH, \"sim-crosswalk.parquet\"))\n",
    "sim_xw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_xw = pd.read_csv(Path(RAW_DATA_FILEPATH, \"baseline-crosswalk.csv\"))\n",
    "baseline_xw.to_parquet(Path(CROSSWALK_FILEPATH, \"baseline-crosswalk.parquet\"))\n",
    "baseline_xw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attributes\n",
    "attr1 = pd.read_csv(Path(RAW_DATA_FILEPATH, \"gage_attr_2yr_discharge.csv\"))\n",
    "attr1.to_parquet(Path(ATTRIBUTE_FILEPATH, \"2yr_discharge.parquet\"))\n",
    "display(attr1)\n",
    "\n",
    "attr2 = pd.read_csv(Path(RAW_DATA_FILEPATH, \"gage_attr_drainage_area_km2.csv\"))\n",
    "attr2.to_parquet(Path(ATTRIBUTE_FILEPATH, \"drainage_area.parquet\"))\n",
    "display(attr2)\n",
    "\n",
    "attr3 = pd.read_csv(Path(RAW_DATA_FILEPATH, \"gage_attr_ecoregion.csv\"))\n",
    "attr3.to_parquet(Path(ATTRIBUTE_FILEPATH, \"ecoregion.parquet\"))\n",
    "display(attr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timeseries\n",
    "obs = pd.read_csv(Path(RAW_DATA_FILEPATH, \"obs.csv\"))\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the other columns required for TEEHR\n",
    "obs['configuration'] = 'usgs'\n",
    "obs['variable_name'] = 'streamflow_daily_mean'\n",
    "obs['measurement_unit'] = 'cms'\n",
    "obs['reference_time'] = None\n",
    "# Reference_time column must be cast as type datetime64[ns] if set to None\n",
    "obs['reference_time'] = obs['reference_time'].astype('datetime64[ns]')\n",
    "obs.to_parquet(Path(PRIMARY_FILEPATH, \"obs.parquet\"))\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ts = pd.read_csv(Path(RAW_DATA_FILEPATH, \"baseline.csv\"))\n",
    "baseline_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the other columns required for TEEHR\n",
    "baseline_ts['configuration'] = 'modeled'\n",
    "baseline_ts['variable_name'] = 'streamflow_daily_mean'\n",
    "baseline_ts['measurement_unit'] = 'cms'\n",
    "baseline_ts['reference_time'] = None\n",
    "# Reference_time column must be cast as type datetime64[ns] if set to None\n",
    "baseline_ts['reference_time'] = (\n",
    "    baseline_ts['reference_time'].astype('datetime64[ns]')\n",
    ")\n",
    "baseline_ts.to_parquet(Path(SECONDARY_FILEPATH, \"baseline.parquet\"))\n",
    "baseline_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ts = pd.read_csv(Path(RAW_DATA_FILEPATH, \"sim.csv\"))\n",
    "sim_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the other columns required for TEEHR\n",
    "sim_ts['configuration'] = 'sim'\n",
    "sim_ts['variable_name'] = 'streamflow_daily_mean'\n",
    "sim_ts['measurement_unit'] = 'cms'\n",
    "sim_ts['reference_time'] = None\n",
    "# Reference_time column must be cast as type datetime64[ns] if set to None\n",
    "sim_ts['reference_time'] = (\n",
    "    baseline_ts['reference_time'].astype('datetime64[ns]')\n",
    ")\n",
    "sim_ts.to_parquet(Path(SECONDARY_FILEPATH, \"sim.parquet\"))\n",
    "sim_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_FILEPATH = f\"{PRIMARY_FILEPATH}/**/*.parquet\"\n",
    "SECONDARY_FILEPATH = f\"{SECONDARY_FILEPATH}/**/*.parquet\"\n",
    "CROSSWALK_FILEPATH = f\"{CROSSWALK_FILEPATH }/**/*.parquet\"\n",
    "GEOMETRY_FILEPATH = f\"{GEOMETRY_FILEPATH }/**/*.parquet\"\n",
    "ATTRIBUTE_FILEPATH = f\"{ATTRIBUTE_FILEPATH}/**/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data\n",
    "if DB_FILEPATH.is_file():\n",
    "    DB_FILEPATH.unlink()\n",
    "\n",
    "db = DuckDBDatabase(DB_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the timeseries data\n",
    "db.insert_joined_timeseries(\n",
    "    primary_filepath=PRIMARY_FILEPATH,\n",
    "    secondary_filepath=SECONDARY_FILEPATH,\n",
    "    crosswalk_filepath=CROSSWALK_FILEPATH,\n",
    "    drop_added_fields=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert geometry\n",
    "db.insert_geometry(GEOMETRY_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert attributes\n",
    "db.insert_attributes(ATTRIBUTE_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.query(f\"\"\"\n",
    "    COPY (\n",
    "        SELECT *\n",
    "        FROM joined_timeseries\n",
    "        ORDER BY configuration, primary_location_id, value_time\n",
    "    )\n",
    "   TO '{JOINED_FILEPATH}/joined.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOINED_FILEPATH = f\"{JOINED_FILEPATH}/**/*.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DuckDBJoinedParquet(JOINED_FILEPATH, GEOMETRY_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jts = db.get_joined_timeseries(\n",
    "    filters=[\n",
    "        {\n",
    "            \"column\": \"primary_location_id\",\n",
    "            \"operator\": \"=\",\n",
    "            \"value\": \"gage-A\"\n",
    "        },\n",
    "    ],\n",
    "    order_by=[\"configuration\", \"primary_location_id\", \"value_time\"],\n",
    ")\n",
    "jts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = db.get_metrics(\n",
    "    group_by=[\"primary_location_id\", \"configuration\"],\n",
    "    order_by=[\"primary_location_id\", \"configuration\"],\n",
    "    include_metrics=\"all\"\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
