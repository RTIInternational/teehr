{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "SPARK_HOME = pyspark.__path__[0]\n",
    "print(f\"SPARK_HOME is: {SPARK_HOME}\")\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "print(f\"SPARK_VERSION is: {SPARK_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Tell Bokeh to output plots in the notebook\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import SedonaContext, ST_GeomFromWKB\n",
    "\n",
    "# config = (\n",
    "#     SedonaContext.builder()\n",
    "#     .appName(\"TEEHR\")\n",
    "#     .master(\"local[*]\")\n",
    "#     .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#     .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "#     .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "#     .config('spark.jars.packages', 'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0,org.datasyslab:geotools-wrapper:1.7.0-28.5,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.524')\n",
    "#     .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all')\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# spark = SedonaContext.create(config)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TEEHR\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config('spark.jars.packages', 'org.apache.sedona:sedona-spark-3.5_2.12:1.7.0,org.datasyslab:geotools-wrapper:1.7.0-28.5,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.524')\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all')\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName)\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# spark = SparkSession. \\\n",
    "#     builder. \\\n",
    "#     appName('Python Spark Apache Sedona example'). \\\n",
    "#     master('local[*]'). \\\n",
    "#     config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "#     config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "#     config('spark.jars.packages',\n",
    "#            'org.apache.sedona:sedona-python-adapter-3.0_2.12:1.0.1-incubating,'\n",
    "#            'org.datasyslab:geotools-wrapper:geotools-24.1'). \\\n",
    "#     getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the Evaluation will be created\n",
    "test_eval_dir = Path(Path().home(), \"temp\", \"10_sedona\")\n",
    "shutil.rmtree(test_eval_dir, ignore_errors=True)\n",
    "\n",
    "# Create an Evaluation object and create the directory\n",
    "ev = teehr.Evaluation(dir_path=test_eval_dir, create_dir=True, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the e0_2_location_example evaluation from the S3 bucket\n",
    "ev.clone_from_s3(\"e0_2_location_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = ev.locations.to_sdf()\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.withColumn(\"geometry2\", ST_GeomFromWKB(\"geometry\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(ev.locations.dir)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ev.spark.read.format(\"geoparquet.metadata\").load(path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
