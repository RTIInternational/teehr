{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a140bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import teehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cbdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEFS_DIR = \"/home/sam/temp/hefs/Workspace/OHRFC/Baseline_validation/haksu.lee.ma/DEP_hindcasts1/Export/RTi_HEFS_Localflows/\"\n",
    "\n",
    "# EVAL_DIR = \"/home/sam/temp/hefs/dwnn6_evaluation\"\n",
    "\n",
    "EVAL_DIR = \"/data/playground/slamont/hefs/dwnn6_evaluation\"   # remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbc1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/15 15:03:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "ev = teehr.Evaluation(dir_path=EVAL_DIR)\n",
    "# ev.enable_logging()\n",
    "\n",
    "constant_field_values = {\n",
    "    \"unit_name\": \"ft^3/s\",\n",
    "    \"variable_name\": \"streamflow_hourly_inst\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.secondary_timeseries.load_fews_xml(\n",
    "    in_path=HEFS_DIR,\n",
    "    constant_field_values=constant_field_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9760733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev.secondary_timeseries.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bef82",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7a970",
   "metadata": {},
   "source": [
    "#### Let's try grouping the cache files into bigger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff6100ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = sorted(list(Path(EVAL_DIR, \"cache\", \"loading\", \"secondary_timeseries\").glob(\"*.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4791f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30bc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [str(path) for path in filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03339a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 1000\n",
    "\n",
    "chunks = [filelist[x:x+chunksize] for x in range(0, len(filelist), chunksize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0fcd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d039cb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d582d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = self.schema_func().to_structtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e0b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp cache dir\n",
    "cache_dir = Path(EVAL_DIR, \"cache\", \"loading\", \"secondary_timeseries_chunked\")\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ccb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"header\": \"true\",\n",
    "    \"ignoreMissingFiles\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c60c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10 done\n",
      "CPU times: user 450 ms, sys: 174 ms, total: 624 ms\n",
      "Wall time: 19min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, file_chunk in enumerate(chunks):\n",
    "\n",
    "    df = ev.spark.read.format(\"parquet\").options(**options).load(file_chunk)\n",
    "    df = df.repartition(1)\n",
    "    (\n",
    "        df.\n",
    "        write.\n",
    "        format(\"parquet\").\n",
    "        mode(\"overwrite\").\n",
    "        save(str(cache_dir) + f\"/chunk_{i}.parquet\")\n",
    "    )\n",
    "    print(f\"Chunk {i} done\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e568b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704af26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
