jupyterhub:
  proxy:
    https:
      enabled: true
      hosts:
        - teehr-hub.rtiamanzi.org
      letsencrypt:
        contactEmail: mdenno@rti.org
  hub:
    config:
      GitHubOAuthenticator:
        client_id: ${GH_OAUTH_CLIENT_ID}
        client_secret: ${GH_OAUTH_CLIENT_SECRET}
        oauth_callback_url: https://teehr-hub.rtiamanzi.org/hub/oauth_callback
        allowed_organizations:
          - rtiinternational:teehr-hub-users
        scope:
          - read:org
      JupyterHub:
        authenticator_class: github
      Authenticator:
        admin_users:
          - mgdenno
          - samlamont
        # allowed_users:
        #   - mgdenno
  singleuser:
    image:
      name: 935462133478.dkr.ecr.us-east-2.amazonaws.com/teehr  # Image to use for singleuser environment. Must include dask-gateway.
      tag: v0.3.17
    defaultUrl: "/lab"  # Use jupyterlab by defualt.
    profileList:
    - display_name: "TEEHR Evaluation System (4 vCPU/32GB memory)"
      default: True
      description: "A r5.xlarge EC2 instance $0.252/hour"
      kubespawner_override:
        mem_limit: 32G
        mem_guarantee: 19G
        cpu_limit: 4
        cpu_guarantee: 3
        # cpu_limit: null
        # mem_limit: null
        node_selector:
          node.kubernetes.io/instance-type: r5.xlarge
    - display_name: "TEEHR Evaluation System (16 vCPU/128GB memory)"
      description: "A r5.4xlarge EC2 instance @ $1.008/hour"
      kubespawner_override:
        mem_limit: 128G
        mem_guarantee: 76G
        cpu_limit: 16
        cpu_guarantee: 12
        # cpu_limit: null
        # mem_limit: null
        node_selector:
          node.kubernetes.io/instance-type: r5.4xlarge
    storage:
      extraVolumes:
      - name: teehr-hub-data-nfs
        persistentVolumeClaim:
            claimName: data-nfs
      extraVolumeMounts:
      - name: teehr-hub-data-nfs
        mountPath: /data

  scheduling:
    userScheduler:
      enabled: true
    # podPriority:
    #   enabled: true
    # userPlaceholder:
    #   enabled: true
    #   replicas: 4
    userPods:
      nodeAffinity:
        matchNodePurpose: require

  cull:
    enabled: true
    timeout: 3600
    every: 300
    adminUsers: false

dask-gateway:
  gateway:
    backend:
      image:
        name: 935462133478.dkr.ecr.us-east-2.amazonaws.com/teehr  # Image to use for singleuser environment. Must include dask-gateway.
        tag: v0.3.17
      scheduler:
        extraPodConfig:
          # serviceAccountName: user-sa
          tolerations:
            # Let's put schedulers on notebook nodes, since they aren't ephemeral
            # dask can recover from dead workers, but not dead schedulers
            - key: "hub.jupyter.org/dedicated"
              operator: "Equal"
              value: "user"
              effect: "NoSchedule"
            - key: "hub.jupyter.org_dedicated"
              operator: "Equal"
              value: "user"
              effect: "NoSchedule"
          nodeSelector:
            k8s.dask.org/node-purpose: scheduler
        cores:
          request: 0.01
          limit: 1
        memory:
          request: 128M
          limit: 1G
      worker:
        extraPodConfig:
          volumes:
            - name: teehr-hub-data-nfs
              persistentVolumeClaim:
                  claimName: data-nfs
#           # serviceAccountName: user-sa
#           # securityContext:
#           #   fsGroup: 1000
          tolerations:
            - key: "k8s.dask.org/dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
            - key: "k8s.dask.org_dedicated"
              operator: "Equal"
              value: "worker"
              effect: "NoSchedule"
          nodeSelector:
            # Dask workers get their own pre-emptible pool
            k8s.dask.org/node-purpose: worker
            node.kubernetes.io/instance-type: r5.4xlarge
        extraContainerConfig:
          volumeMounts:
            - name: teehr-hub-data-nfs
              mountPath: /data
        #   securityContext:
        #     runAsGroup: 1000
        #     runAsUser: 1000

    # TODO: figure out a replacement for userLimits.
    extraConfig:
      optionHandler: |
        from dask_gateway_server.options import Options, Integer, Float, String, Mapping
        import string

        # Escape a string to be dns-safe in the same way that KubeSpawner does it.
        # Reference https://github.com/jupyterhub/kubespawner/blob/616f72c4aee26c3d2127c6af6086ec50d6cda383/kubespawner/spawner.py#L1828-L1835
        # Adapted from https://github.com/minrk/escapism to avoid installing the package
        # in the dask-gateway api pod which would have been problematic.
        def escape_string_label_safe(to_escape):
            safe_chars = set(string.ascii_lowercase + string.digits)
            escape_char = "-"
            chars = []
            for c in to_escape:
                if c in safe_chars:
                    chars.append(c)
                else:
                    # escape one character
                    buf = []
                    # UTF-8 uses 1 to 4 bytes per character, depending on the Unicode symbol
                    # so we need to transform each byte to its hex value
                    for byte in c.encode("utf8"):
                        buf.append(escape_char)
                        # %X is the hex value of the byte
                        buf.append('%X' % byte)
                    escaped_hex_char = "".join(buf)
                    chars.append(escaped_hex_char)
            return u''.join(chars)

        def cluster_options(user):
            safe_username = escape_string_label_safe(user.name)
            def option_handler(options):
                #if ":" not in options.image:
                #    raise ValueError("When specifying an image you must also provide a tag")
                extra_annotations = {
                    "hub.jupyter.org/username": safe_username,
                    "prometheus.io/scrape": "true",
                    "prometheus.io/port": "8787",
                }
                extra_labels = {
                    "hub.jupyter.org/username": safe_username,
                }
                return {
                    "worker_cores_limit": options.worker_cores,
                    "worker_cores": options.worker_cores,
                    "worker_memory": "%fG" % options.worker_memory,
                    #"image": options.image,
                    "scheduler_extra_pod_annotations": extra_annotations,
                    "worker_extra_pod_annotations": extra_annotations,
                    "scheduler_extra_pod_labels": extra_labels,
                    "worker_extra_pod_labels": extra_labels,
                    #"environment": options.environment,
                }
            return Options(
                Integer("worker_cores", 2, min=1, label="Worker Cores"),
                Float("worker_memory", 4, min=1, label="Worker Memory (GiB)"),
                # The default image is set via DASK_GATEWAY__CLUSTER__OPTIONS__IMAGE env variable
                # String("image", label="Image"),
                # Mapping("environment", {}, label="Environment Variables"),
                handler=option_handler,
            )
        c.Backend.cluster_options = cluster_options
      idle: |
        # timeout after 30 minutes of inactivity
        c.KubeClusterConfig.idle_timeout = 1800
    prefix: "/services/dask-gateway" # Users connect to the Gateway through the JupyterHub service.
    auth:
      type: jupyterhub # Use JupyterHub to authenticate with Dask Gateway
  # traefik:
  #   nodeSelector:
  #     k8s.dask.org/node-purpose: core
  #   service:
  #     type: ClusterIP # Access Dask Gateway through JupyterHub. To access the Gateway from outside JupyterHub, this must be changed to a `LoadBalancer`.
